# A Research of Optimizer Scheduling

## Experimental Results

### Adam

<div align="center">
    <img src="./assets/adam_sch_dis_0.01.png" /><br />
    <i>Adam with Fixed Learning Rate (lr=0.01)</i>
</div><br />

<div align="center">
    <img src="./assets/adam_sch_en.png" /><br />
    <i>Adam with Scheduled Learning Rate</i>
</div><br />

### RAdam

<div align="center">
    <img src="./assets/radam_sch_dis_0.03.png" /><br />
    <i>RAdam with Fixed Learning Rate (lr=0.03)</i>
</div><br />

### SGD

<div align="center">
    <img src="./assets/sgd_sch_dis_0.003.png" /><br />
    <i>SGD with Fixed Learning Rate (lr=0.003)</i>
</div><br />

<div align="center">
    <img src="./assets/sgd_sch_dis_0.03.png" /><br />
    <i>SGD with Fixed Learning Rate (lr=0.03)</i>
</div><br />

### Adagrad

<div align="center">
    <img src="./assets/adagrad_sch_dis_0.003.png" /><br />
    <i>Adagrad with Fixed Learning Rate (lr=0.003)</i>
</div><br />

<div align="center">
    <img src="./assets/adagrad_sch_dis_0.03.png" /><br />
    <i>Adagrad with Fixed Learning Rate (lr=0.03)</i>
</div><br />